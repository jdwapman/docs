
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="icon" type="image/ico" href="images/favicon.ico">
    <title>HIVE Year 1 Report&colon; Executive Summary</title>

    <script type="text/javascript" src="https://code.jquery.com/jquery-2.2.4.min.js"></script>
    <style>
      .highlight table td { padding: 5px; }
.highlight table pre { margin: 0; }
.highlight .gh {
  color: #999999;
}
.highlight .sr {
  color: #f6aa11;
}
.highlight .go {
  color: #888888;
}
.highlight .gp {
  color: #555555;
}
.highlight .gs {
}
.highlight .gu {
  color: #aaaaaa;
}
.highlight .nb {
  color: #f6aa11;
}
.highlight .cm {
  color: #75715e;
}
.highlight .cp {
  color: #75715e;
}
.highlight .c1 {
  color: #75715e;
}
.highlight .cs {
  color: #75715e;
}
.highlight .c, .highlight .cd {
  color: #75715e;
}
.highlight .err {
  color: #960050;
}
.highlight .gr {
  color: #960050;
}
.highlight .gt {
  color: #960050;
}
.highlight .gd {
  color: #49483e;
}
.highlight .gi {
  color: #49483e;
}
.highlight .ge {
  color: #49483e;
}
.highlight .kc {
  color: #66d9ef;
}
.highlight .kd {
  color: #66d9ef;
}
.highlight .kr {
  color: #66d9ef;
}
.highlight .no {
  color: #66d9ef;
}
.highlight .kt {
  color: #66d9ef;
}
.highlight .mf {
  color: #ae81ff;
}
.highlight .mh {
  color: #ae81ff;
}
.highlight .il {
  color: #ae81ff;
}
.highlight .mi {
  color: #ae81ff;
}
.highlight .mo {
  color: #ae81ff;
}
.highlight .m, .highlight .mb, .highlight .mx {
  color: #ae81ff;
}
.highlight .sc {
  color: #ae81ff;
}
.highlight .se {
  color: #ae81ff;
}
.highlight .ss {
  color: #ae81ff;
}
.highlight .sd {
  color: #e6db74;
}
.highlight .s2 {
  color: #e6db74;
}
.highlight .sb {
  color: #e6db74;
}
.highlight .sh {
  color: #e6db74;
}
.highlight .si {
  color: #e6db74;
}
.highlight .sx {
  color: #e6db74;
}
.highlight .s1 {
  color: #e6db74;
}
.highlight .s {
  color: #e6db74;
}
.highlight .na {
  color: #a6e22e;
}
.highlight .nc {
  color: #a6e22e;
}
.highlight .nd {
  color: #a6e22e;
}
.highlight .ne {
  color: #a6e22e;
}
.highlight .nf {
  color: #a6e22e;
}
.highlight .vc {
  color: #ffffff;
}
.highlight .nn {
  color: #ffffff;
}
.highlight .nl {
  color: #ffffff;
}
.highlight .ni {
  color: #ffffff;
}
.highlight .bp {
  color: #ffffff;
}
.highlight .vg {
  color: #ffffff;
}
.highlight .vi {
  color: #ffffff;
}
.highlight .nv {
  color: #ffffff;
}
.highlight .w {
  color: #ffffff;
}
.highlight {
  color: #ffffff;
}
.highlight .n, .highlight .py, .highlight .nx {
  color: #ffffff;
}
.highlight .ow {
  color: #f92672;
}
.highlight .nt {
  color: #f92672;
}
.highlight .k, .highlight .kv {
  color: #f92672;
}
.highlight .kn {
  color: #f92672;
}
.highlight .kp {
  color: #f92672;
}
.highlight .o {
  color: #f92672;
}
    </style>
    <link href="stylesheets/screen.css" rel="stylesheet" media="screen" />
    <link href="stylesheets/print.css" rel="stylesheet" media="print" />
      <script src="javascripts/all.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/vega@3"></script>
    <script src="https://cdn.jsdelivr.net/npm/vega-lite@2"></script>
    <script src="https://cdn.jsdelivr.net/npm/vega-embed@3"></script>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
    </script>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <style media="screen">
      /* Add space between vega-embed links */
      /* http://vega.github.io/vega-tutorials/airports/ */
      .vega-embed .vega-actions a {
        margin-left: 1em;
        visibility: hidden;
      }
      .vega-embed:hover .vega-actions a {
        visibility: visible;
      }
    </style>
  </head>

  <body class="hive_year1_summary" data-languages="[]">
    <a href="#" id="nav-button">
      <span>
        NAV
        <img src="images/navbar.png" alt="Navbar" />
      </span>
    </a>
    <div class="toc-wrapper">
      <img src="images/logo.png" class="logo" alt="Logo" />
        <div class="search">
          <input type="text" class="search" id="input-search" placeholder="Search">
        </div>
        <ul class="search-results"></ul>
      <div id="toc" class="toc-list-h1">
          <li>
            <a href="#hive-year-1-report-colon-executive-summary" class="toc-h1 toc-link" data-title="HIVE Year 1 Report&amp;colon; Executive Summary">HIVE YEAR 1 REPORT&AMP;COLON; EXECUTIVE SUMMARY</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#application-classification" class="toc-h2 toc-link" data-title="Application Classification">Application Classification</a>
                  </li>
                  <li>
                    <a href="#geolocation" class="toc-h2 toc-link" data-title="Geolocation">Geolocation</a>
                  </li>
                  <li>
                    <a href="#graphsage" class="toc-h2 toc-link" data-title="GraphSAGE">GraphSAGE</a>
                  </li>
                  <li>
                    <a href="#graphsearch" class="toc-h2 toc-link" data-title="GraphSearch">GraphSearch</a>
                  </li>
                  <li>
                    <a href="#community-detection-louvain" class="toc-h2 toc-link" data-title="Community Detection (Louvain)">Community Detection (Louvain)</a>
                  </li>
                  <li>
                    <a href="#local-graph-clustering-lgc" class="toc-h2 toc-link" data-title="Local Graph Clustering (LGC)">Local Graph Clustering (LGC)</a>
                  </li>
                  <li>
                    <a href="#graph-projections" class="toc-h2 toc-link" data-title="Graph Projections">Graph Projections</a>
                  </li>
              </ul>
          </li>
      </div>
        <ul class="toc-footer">
            <li><a href='https://github.com/gunrock/gunrock'>Gunrock&colon; GPU Graph Analytics</a></li>
            <li>Gunrock &copy; 2018 The Regents of the University of California.</li>
        </ul>
    </div>
    <div class="page-wrapper">

        <!-- <div class="dark-box"></div> -->

      <div class="content">
        <h1 id='hive-year-1-report-colon-executive-summary'>HIVE Year 1 Report&colon; Executive Summary</h1>
<p>This report is located online at the following URL: <a href="https://gunrock.github.io/docs/hive_year1_summary.html">https://gunrock.github.io/docs/hive_year1_summary.html</a>.</p>

<p>Herein UC Davis produces the following three deliverables that it promised to deliver in Year 1:</p>

<ol>
<li><strong>7--9 kernels running on a single GPU on DGX-1</strong>. The PM had indicated that the application targets are the graph-specific kernels of larger applications, and that our effort should target these kernels. These kernels run on one GPU of the DGX-1. These kernels are in Gunrock's GitHub repository as standalone kernels. While we committed to delivering 7--9 kernels, we deliver 10 v0 kernels. Scan statistics is substantially done but the report is not complete and so we do not deliver it. Sparse graph lasso works on some inputs but requires more optimization in its maxflow component; we do include its report.</li>
<li><strong>(High-level) performance analysis of these kernels</strong>. In this report we analyze the performance of these kernels.</li>
<li><strong>Separable communication benchmark predicting latency and throughput for a multi-GPU implementation</strong>. This report (and associated code, also in the Gunrock GitHub repository) analyzes the DGX-1's communication capabilities and projects how single-GPU benchmarks will scale on this machine to 8 GPUs.</li>
</ol>

<p>Specific notes on applications and scaling follow:</p>
<h2 id='application-classification'>Application Classification</h2>
<p><strong><a href="https://gunrock.github.io/docs/hive_application_classification.html">Application Classification</a></strong> 
Application classification involves a number of dense-matrix operations, which did not make it an obvious candidate for implementation in Gunrock.  However, our GPU implementation using the CUDA CUB library shows substantial speedups (10-50x) over the multi-threaded OpenMP implementations.</p>

<p>However, there are two neighbor reduce operations that may benefit from the kind of load balancing implemented in Gunrock.  Thus, it would be useful to either expose lightweight wrappers of high-performance Gunrock primitives for easy intergration into outside projects <em>or</em> come up with a workflow inside of Gunrock that makes programming applications with lots of non-graph operations straightforward.</p>
<h2 id='geolocation'>Geolocation</h2>
<p><strong><a href="https://gunrock.github.io/docs/hive_geolocation.html">Geolocation</a></strong> 
Geolocation or geotagging is an interesting parallel problem, because it is among the few that exhibits the dynamic parallism pattern within the compute. The pattern is as follows; there is parallel compute across nodes, each node has some serial work and within the serial work there are sveral parallel math operations. Even without leveraging dynamic parallelism within CUDA (kernel launches within a kernel), Geolocation performs well on the GPU environment because it mainly requires simple math operations, instead of complicated memory movement schemes. </p>

<p>However, the challenge within the application is load balancing this simple compute, such that each processor has roughly the same amount of work. Currently, in gunrock, we map Geolocation using the <code>ForAll()</code> compute operator with optimizations to exit early (performing less work and fewer reads). Even without addressing load balancing issue with a complicated balancing scheme, on the HIVE datasets we achieve a 100x speedup with respect to the CPU reference code, implemented using C++ and OpenMP, and ~533x speedup  with respect to the GTUSC implementation. We improve upon the algorithm by avoiding a global gather and a global synchronize, and using 3x less memory than the GTUSC reference implementation.</p>
<h2 id='graphsage'>GraphSAGE</h2>
<p><strong><a href="https://gunrock.github.io/docs/hive_graphSage.html">GraphSAGE</a></strong> 
The vertex embedding part of the GraphSAGE algorithm is implemented in the
Gunrock framework using custom CUDA kernels, utilizing block-level
parallelism, that allow a shorter running time. For the embedding part alone, the GPU
implementation is 7.5X to 15X on P100, and 20X to 30X on V100,
faster than an OpenMP implementation using 32 threads. The GPU hardware, especially
the memory system, has high utilizations from these custom kernels. It is still
unclear how to expose block-level parallelism for more general usage in
other applications in Gunrock.</p>

<p>Connecting the vertex embedding with the neural network training part, and
making the GraphSAGE workflow complete, would be an interesting task for year 2.
Testing on the complete workflow for prediction accuracy and running speed will
be more meaningful.</p>
<h2 id='graphsearch'>GraphSearch</h2>
<p><strong><a href="https://gunrock.github.io/docs/hive_graphsearch.html">GraphSearch</a></strong> 
Graph search is a relatively minor modification to Gunrock's random walk application, and was straightforward to implement.  Though random walks are a "worst case scenario" for GPU memory bandwidth, we still achieve 3--5x speedup over a modified version of the OpenMP reference implementation.</p>

<p>The original OpenMP reference implementation actually ran slower with more threads -- we fixed the bugs, but the benchmarking experience highlights the need for performant and hardened CPU baselines.</p>

<p>Until recently, Gunrock did not support parallelism <em>within</em> the lambda functions run by the <code>advance</code> operator, so neighbor selection for a given step in the walk is done sequentially.  Methods for exposing more parallelism to the programmer are currently being developed via parallel neighbor reduce functions.</p>

<p>In an end-to-end graph search application, we'd need to implement the scoring function as well as the graph walk component.  For performance, we'd likely want to implement the scoring function on the GPU as well, which makes this a good example of a "Gunrock+X" app, where we'd need to integrate the high-performance graph processing component with arbitrary user code.</p>
<h2 id='community-detection-louvain'>Community Detection (Louvain)</h2>
<p><strong><a href="https://gunrock.github.io/docs/hive_louvain.html">Community Detection (Louvain)</a></strong> 
The Gunrock implementation uses sort and segmented reduce to implement the
Louvain algorithm, different from the commonly used hash table mapping. The GPU
implementation is about ~1.5X faster than the OpenMP implementation, and also
faster than previous GPU works. It is still unknown whether the sort and
segmented reduce formulation map the problem better than hash table on the GPU. The
modularities resulting from the GPU implementation are within small differences
as the serial implementation, and are better when the graph is larger. A custom
hash table can potentially improve the running time. The GPU Louvain
implementation should have moderate scalability across multiple GPUs in an
DGX-1.</p>
<h2 id='local-graph-clustering-lgc'>Local Graph Clustering (LGC)</h2>
<p><strong><a href="https://gunrock.github.io/docs/hive_pr_nibble.html">Local Graph Clustering (LGC)</a></strong> 
This variant of local graph clustering (L1 regularized PageRank via FISTA) is a natural fit for Gunrock's frontier-based programming paradigm.  We observe speedups of 2-3 orders of magnitude over the HIVE reference implementation.</p>

<p>The reference implementation of the algorithm was not explicitly written as <code>advance</code>/<code>filter</code>/<code>compute</code> operations, but we were able to quickly determine how to map the operations by using <a href="https://github.com/gunrock/pygunrock/blob/master/apps/pr_nibble.py">a lightweight Python implementation of the Gunrock programming API</a> as a development environment.  Thus, LGC was a good exercise in implementing a non-trivial end-to-end application in Gunrock from scratch.</p>
<h2 id='graph-projections'>Graph Projections</h2>
<p><strong><a href="https://gunrock.github.io/docs/hive_proj.html">Graph Projections</a></strong> 
Because it has a natural representation in terms of sparse matrix operations, graph projections gave us an opportunity to compare ease of implementation and performance between Gunrock and another UC-Davis project, GPU <a href="https://github.com/owensgroup/GraphBLAS">GraphBLAS</a>.  </p>

<p>Overall, we found that Gunrock was more flexible and more performant than GraphBLAS, likely due to better load balancing.  However, in this case, the GraphBLAS application was substantially easier to program than Gunrock, and also allowed us to take advantage of some more sophisticated memory allocation methods available in the GraphBLAS cuSPARSE backend.  These findings suggest that addition of certain commonly used API functions to Gunrock could be a fruitful direction for further work.</p>

      </div>

        <!-- <div class="dark-box"></div> -->
    </div>
  </body>
</html>
